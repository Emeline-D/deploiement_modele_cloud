{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Projet8_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBubGejYJ6Ls",
        "colab_type": "text"
      },
      "source": [
        "<h1>Projet 8 du parcours \"Data Scientist\" : Déployez un modèle dans le cloud</h1>\n",
        "\n",
        "<h2>Sommaire</h2>\n",
        "\n",
        "\n",
        "1) Introduction et présentation de l'architecture utilisée<br/>\n",
        "2) Préprocessing et réduction de dimension des données\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOb8-qrjJ6Lt",
        "colab_type": "text"
      },
      "source": [
        "<h2>Introduction et présentation de l'architecture utilisée</h2>\n",
        "\n",
        "Voici mon projet 8 du parcours \"Data Scientist\", à savoir le déploiement d'un modèle dans le cloud. Ainsi, nous allons déployer un modèle de réduction de dimensions sur des images de fruits sur AWS, dans le but de pouvoir traiter un nombre très important d'images sans faire face à des limitations. Ici, nous avons ainsi un jeu de données limité pour effectuer notre modèle, mais ce jeu pourrait être 100 à 1000 fois plus important dans le futur.<br/>\n",
        "\n",
        "Pour ce faire, nous utilisons donc une architecture de calculs distribués, composée de :\n",
        "- Un espace de stockage S3 contenant les différentes images du projet réparties en 3 dossiers : Traning, Test, et multiple_test-fruits. Ici, pour entraîner le modèle, nous nous servirons exclusivement des images Test.\n",
        "- Un serveur de calculs EC2 qui permettra d'exécuter notre modèle, et lié au S3 grâce à Hadoop. Ici, nous avons choisi une instance payante, mais de petite taille, à savoir t2.xlarge, pour à la fois faire tourner rapidement notre modèle, mais également prévoir un peu de marge avec l'augmentation future du nombre d'images.\n",
        "\n",
        "Pour permettre le bon fonctionnement de notre EC2, il est tout d'abord nécessaire d'installer quelques packages et librairies :\n",
        "- Java version 8\n",
        "- Python 3.6 et pip version python 3\n",
        "- Différentes librairies Python3 : \n",
        " - numpy\n",
        " - pandas\n",
        " - jupyter pour créer ce notebook\n",
        " - tensorflow de version >2. Ici, nous avons choisi la 2.1.0 qui est une version stable du package\n",
        " - pyarrow de version comprise entre 0.10 et 0.14 (ici la 0.13) pour correspondre aux prérequis de Tensorflow et de Pyspark.\n",
        " - Spark et Hadoop (ici Spark 2.4.5 prebuilt for Hadoop 2.7) dans une version stable et fonctionnant avec notre version de Java\n",
        "Après ces installations, les variables d'environnement de Java, de Spark, de Pyspark et d'ouverture du notebook par PySpark ont été définies dans ~/.bashrc.\n",
        "\n",
        "Pour lancer ce notebook, il faut ainsi lancer Pyspark en utilisant ces paramètres :\n",
        "\n",
        "`$SPARK_HOME/bin/pyspark --master local[*,4] --packages org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.7.4`\n",
        "\n",
        "--master local[*,4] correspond à un lancement de Pyspark en local, sur tous les coeurs du processeur disponibles et jusqu'à 4 fois en cas d'erreur. Les deux packages indiqués, Hadoop et Java, sont ceux nécessaires à la lecture des fichiers de AWS S3 dans Spark : ici nous avons choisi respectivement les versions 2.7.3 et 1.7.4 pour des raisons de compatibilité. \n",
        "\n",
        "Une fois cette commande lancée, nous pouvons utiliser Jupyter dans le navigateur, en utilisant le DNS public de l'instance EC2 et le port 8888\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIc8zRheJ6Lu",
        "colab_type": "text"
      },
      "source": [
        "<h2>Préprocessing et réduction de dimension des données</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CF6X941J6Lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Find the localisation of Spark/PySpark on EC2 local server\n",
        "import findspark \n",
        "findspark.init()\n",
        "\n",
        "from subprocess import call\n",
        "import copy\n",
        "\n",
        "# Import of ML and Data Science necessary librairies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from pyspark.sql.functions import col, pandas_udf, lit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fldosxy_J6Ly",
        "colab_type": "text"
      },
      "source": [
        "Après avoir importé les librairies nécessaires et dont nous avons parlé auparavant, il est nécessaire d'instancier une session de Spark (ici Pyspark vu qu nous sommes en Python), à la fois pour exécuter notre modèle, mais aussi déjà pour importer nos images stockées sur S3. Plusieures données sont ainsi à considérer :\n",
        "- indiquer les clés d'accès et secrètes afin d'accéder aux différents espaces (ici retirées pour des raisons de sécurité)\n",
        "- indiquer l'emplacement du dossier dans le bucket S3 contenant les différentes images\n",
        "- limiter la mémoire RAM utilisée afin qu'elle ne sature pas et ne fasse pas planter notre exécution. Pour le nombre actuel d'images utilisées, cela ne sera pas important, mais nous préférons être prévoyant dans le cadre de l'augmentation des données à traiter.\n",
        "- \"com.amazonaws.services.s3.enableV4\" est nécessaire dans le cadre de l'import de données depuis S3 provenant de la région eu-west-3, à savoir Paris.\n",
        "- Les autres paramètres suivant la configuration de la session permettent là encore l'impotration du stockage S3 dans EC2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCyDkTevJ6Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the keys to access to S3\n",
        "AWSAccessKeyId='yourkey'\n",
        "AWSSecretKey='yourkey'\n",
        "\n",
        "# Indicate the localisation of S3 bucket\n",
        "S3dir = 's3a://projet8bucket'\n",
        "\n",
        "#Creation of the Spark Session \n",
        "spark = SparkSession.builder.config(\"spark.python.worker.reuse\", \"False\") \\\n",
        "                            .appName('dim_red') \\\n",
        "                            .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AWSAccessKeyId)\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", AWSSecretKey)\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.eu-west-3.amazonaws.com\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABBT86nMJ6L1",
        "colab_type": "text"
      },
      "source": [
        "Nous importons maintenant nos images dans un Spark Dataframe : ainsi, nous récupérons toutes les données en format image contenues dans le dossier \"Test\", et ce de manière récursive car nos images sont situées elles-mêmes dans des dossiers contenus dans \"Test\".\n",
        "\n",
        "Nous redimensionnons nos images dans des dimensions égales, et surtout compatibles avec le modèle pré-entraîné que nous allons utiliser pour notre réduction de dimension, à savoir MobileNetV2. Ce modèle est en effet simple d'utilisation (il suffit de l'importer depuis Tensorflow) et est rapide tout en donnant de bons résultats. Des modèles comme InceptionResNetV2 auraient également pu être utilisés dans notre cas, mais ce modèle est plus lourd et aurait demandé une plus grosse infrastructure de EC2 : nous avons donc choisi ce compromis pour des raisons de coûts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84qEmFd_J6L2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load images from S3 bucket to a Spark Dataframe\n",
        "df = spark.read.format('image').load(S3dir + '/Test/*/*', recursive=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQgUCTlQJ6L4",
        "colab_type": "code",
        "colab": {},
        "outputId": "4d4ce91a-3c95-4eb9-bffb-a6f42ca49905"
      },
      "source": [
        "# Resize all images to required dimension of MobileNetV2 model\n",
        "IMAGE_SIZE = 96\n",
        "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "\n",
        "# Import MobileNetV2 model, but only the first layers (last used for other images)\n",
        "model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, \n",
        "                                          include_top=False, \n",
        "                                          weights='imagenet')\n",
        "\n",
        "# Save the model locally for future use\n",
        "model.save('models', save_format='tf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ec2-user/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: models/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAeyU58dJ6L8",
        "colab_type": "text"
      },
      "source": [
        "Nous devons maintenant définir tous les éléments de transformation des images, pour une exécution future. En effet, Pyspark n'exécute le code que lors d'un appel à l'action qui se situe à la fin du code. \n",
        "\n",
        "La fonction de réduction de dimensions va ainsi s'effectuer en plusieurs étapes :\n",
        "- Import de l'image\n",
        "- Transformation des différentes variables d'images obtenues dans le Spark Dataframe (width, height et nChannels) en des variables compatibles avec l'utilisation du modèle. D'autres solutions auraient pu être utilisées (transformation directe par une fonction de Spark par exemple), mais des erreurs nous ont contraints à coder ceci par nous-mêmes\n",
        "- Ajout des images transformées dans l'entrée du modèle par la fonction \"preprocess_input()\"\n",
        "- Redimensionnement des images aux dimensions indiquées précédemment\n",
        "- Réduction de dimensions grâce aux premières couches sélectionnés du modèle sauvegardé et obtention des features d'intérêt\n",
        "- Conversion de la liste des features en Series Pandas\n",
        "\n",
        "Cette fonction classique est ensuite transformée en fonction de pandas_udf, afin de pouvoir exécuter la fonction de manière parallélisée (plusieurs images traitées en même temps)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK4Xsy7TJ6L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to transform the images, apply the dimesnion reduction and\n",
        "# obtain the features with the saved model \n",
        "def red_dim(width, height, nChannels, data):\n",
        "    # Transformation of Spark image data to Tensorflow format\n",
        "    images = []\n",
        "    for i in range(height.shape[0]):\n",
        "        x = np.ndarray(\n",
        "                shape=(height[i], width[i], nChannels[i]),\n",
        "                dtype=np.uint8,\n",
        "                buffer=data[i],\n",
        "                strides=(width[i] * nChannels[i], nChannels[i], 1))\n",
        "        images.append(preprocess_input(x))\n",
        "    # Resize images\n",
        "    images = np.array(tf.image.resize(images, [IMAGE_SIZE, IMAGE_SIZE]))\n",
        "    # Load and apply the model\n",
        "    model = load_model('models')\n",
        "    preds = model.predict(images).reshape(len(width), 3 * 3 * 1280)\n",
        "    \n",
        "    # Return a Pandas Series with all features images \n",
        "    return pd.Series(list(preds))\n",
        "\n",
        "# Transformation into a pandas_udf function\n",
        "red_dim_udf = pandas_udf(red_dim, returnType=ArrayType(DoubleType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUhVtiA3J6L_",
        "colab_type": "text"
      },
      "source": [
        "Une dernière variable a besoin d'être initialisée : celle permettant de connaître le temps au lancement du programme (du jour à la seconde). Ceci nous sera utile pour créer le dossier contenant les différents fichiers finaux de notre sortie de modèle, afin d'éviter les doublons et ainsi le plantage de notre programme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm7LENvMJ6MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "today = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYozg1YsJ6MD",
        "colab_type": "text"
      },
      "source": [
        "Maintenant que tout a été initialisé et la fonction de transformation écrite, il ne reste plus qu'à faire un appel à action pour exécuter tout le code précédent. On créée ainsi une colonne dans notre Dataframe contenant toutes les features de chaque image, et une autre contenant ces features converties en string pour être compatibles avec un enregistrement en fichier CSV. Nous sélectionnons ensuite uniquement la colonne contenant le lien vers les images (et ainsi leur nom) et celle contenant les features converties.\n",
        "\n",
        "Ce dataframe contenant maintenant deux colonnes va enfin être enregistré dans 5 fichiers CSV différents en parallèle : en effet, un unique fichier sera très lourd et donc plus long à traiter par la suite que plusieurs, et 350Mo environ pour chaque fichier semble un bon compromis entre le nombre de fichiers et leur poids. Ces fichiers sont contenus dans un dossier du bucket S3. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HoYtYfkJ6MD",
        "colab_type": "code",
        "colab": {},
        "outputId": "a6001985-dfca-48e9-b503-873274e0eee5"
      },
      "source": [
        "# Launch the action of Pyspark application\n",
        "results=df.withColumn(\"dim_red\", red_dim_udf(col(\"image.width\"), col(\"image.height\"), \\\n",
        "                                                    col(\"image.nChannels\"), \\\n",
        "                                                    col(\"image.data\"))) \\\n",
        "                 .withColumn(\"dim_red_string\", lit(col(\"dim_red\").cast(\"string\"))) \\\n",
        "                 .select(\"image.origin\", 'dim_red_string')\\\n",
        "                 .repartition(5).write.csv(S3dir + '/results' + today)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"print(results.count())\\n\\nresults.repartition(5).write.csv(S3dir + '/results' + today)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WyT3_50J6MG",
        "colab_type": "text"
      },
      "source": [
        "Ainsi, nous avons maintenant nos 5 fichiers prêts à être traités pour calculer les différentes prédictions de fruits dans le futur. Une amélioration possible serait par exemple d'utiliser une instance EMR avec l'instance EC2 car cela serait une méthode plus actuelle, mais un blocage du groupe de sécurité de notre compte a rendu ceci impossible sans créer un nouveau compte AWS, l'idée a donc été pour le moment abandonnée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBVX_sRSJ6MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}